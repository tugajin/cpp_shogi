{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "_dkfRUrkjMui"
   },
   "outputs": [],
   "source": [
    "# environment:\n",
    "# pip3 install torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "vip06874jMul"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 2 3\n",
      "A _ _ _\n",
      "B O _ _\n",
      "C _ _ _\n",
      "record = B1\n",
      "input feature\n",
      "[[[0. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [1. 0. 0.]\n",
      "  [0. 0. 0.]]]\n",
      "input feature\n",
      "[[[1. 0. 0.]\n",
      "  [0. 0. 0.]\n",
      "  [0. 0. 0.]]\n",
      "\n",
      " [[0. 0. 0.]\n",
      "  [0. 1. 0.]\n",
      "  [0. 1. 0.]]]\n"
     ]
    }
   ],
   "source": [
    "# Implementation of simple game: Tic-Tac-Toe\n",
    "# You can change this to another two-player game.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "BLACK, WHITE =1, -1 # first turn or second turn player\n",
    "\n",
    "class State:\n",
    "    '''Board implementation of Tic-Tac-Toe'''\n",
    "    X, Y = 'ABC',  '123'\n",
    "    C = {0: '_', BLACK: 'O', WHITE: 'X'}\n",
    "\n",
    "    def __init__(self):\n",
    "        self.board = np.zeros((3, 3)) # (x, y)\n",
    "        self.color = 1\n",
    "        self.win_color = 0\n",
    "        self.record = []\n",
    "\n",
    "    def action2str(self, a):\n",
    "        return self.X[a // 3] + self.Y[a % 3]\n",
    "\n",
    "    def str2action(self, s):\n",
    "        return self.X.find(s[0]) * 3 + self.Y.find(s[1])\n",
    "\n",
    "    def record_string(self):\n",
    "        return ' '.join([self.action2str(a) for a in self.record])\n",
    "\n",
    "    def __str__(self):\n",
    "        # output board.\n",
    "        s = '   ' + ' '.join(self.Y) + '\\n'\n",
    "        for i in range(3):\n",
    "            s += self.X[i] + ' ' + ' '.join([self.C[self.board[i, j]] for j in range(3)]) + '\\n'\n",
    "        s += 'record = ' + self.record_string()\n",
    "        return s\n",
    "\n",
    "    def play(self, action):\n",
    "        # state transition function\n",
    "        # action is position inerger (0~8) or string representation of action sequence\n",
    "        if isinstance(action, str):\n",
    "            for astr in action.split():\n",
    "                self.play(self.str2action(astr))\n",
    "            return self\n",
    "\n",
    "        x, y = action // 3, action % 3\n",
    "        self.board[x, y] = self.color\n",
    "\n",
    "        # check whether 3 stones are on the line\n",
    "        if self.board[x, :].sum() == 3 * self.color \\\n",
    "          or self.board[:, y].sum() == 3 * self.color \\\n",
    "          or (x == y and np.diag(self.board, k=0).sum() == 3 * self.color) \\\n",
    "          or (x == 2 - y and np.diag(self.board[::-1,:], k=0).sum() == 3 * self.color):\n",
    "            self.win_color = self.color\n",
    "\n",
    "        self.color = -self.color\n",
    "        self.record.append(action)\n",
    "        return self\n",
    "\n",
    "    def terminal(self):\n",
    "        # terminal state check\n",
    "        return self.win_color != 0 or len(self.record) == 3 * 3\n",
    "\n",
    "    def terminal_reward(self):\n",
    "        # terminal reward \n",
    "        return self.win_color if self.color == BLACK else -self.win_color\n",
    "\n",
    "    def legal_actions(self):\n",
    "        # list of legal actions on each state\n",
    "        return [a for a in range(3 * 3) if self.board[a // 3, a % 3] == 0]\n",
    "\n",
    "    def feature(self):\n",
    "        # input tensor for neural nets (state)\n",
    "        return np.stack([self.board == self.color, self.board == -self.color]).astype(np.float32)\n",
    "\n",
    "    def action_feature(self, action):\n",
    "        # input tensor for neural nets (action)\n",
    "        a = np.zeros((1, 3, 3), dtype=np.float32)\n",
    "        a[0, action // 3, action % 3] = 1\n",
    "        return a\n",
    "\n",
    "state = State().play('B1')\n",
    "print(state)\n",
    "print('input feature')\n",
    "print(state.feature())\n",
    "state = State().play('B2 A1 C2')\n",
    "print('input feature')\n",
    "print(state.feature())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "State().terminal_reward()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "F72fNyxSjMup"
   },
   "outputs": [],
   "source": [
    "# Neural nets with PyTorch\n",
    "# small version of nets used in MuZero paper\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class Conv(nn.Module):\n",
    "    def __init__(self, filters0, filters1, kernel_size, bn=False):\n",
    "        super().__init__()\n",
    "        self.conv = nn.Conv2d(filters0, filters1, kernel_size, stride=1, padding=kernel_size//2, bias=False)\n",
    "        self.bn = None\n",
    "        if bn:\n",
    "            self.bn = nn.BatchNorm2d(filters1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = self.conv(x)\n",
    "        if self.bn is not None:\n",
    "            h = self.bn(h)\n",
    "        return h\n",
    "\n",
    "class ResidualBlock(nn.Module):\n",
    "    def __init__(self, filters):\n",
    "        super().__init__()\n",
    "        self.conv = Conv(filters, filters, 3, True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return F.relu(x + (self.conv(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BESoEV3sjMus"
   },
   "outputs": [],
   "source": [
    "num_filters = 8\n",
    "num_blocks = 2\n",
    "\n",
    "class Representation(nn.Module):\n",
    "    ''' Conversion from observation to inner abstract state '''\n",
    "    def __init__(self, input_shape):\n",
    "        super().__init__()\n",
    "        self.input_shape = input_shape\n",
    "        self.board_size = self.input_shape[1] * self.input_shape[2]\n",
    "\n",
    "        self.layer0 = Conv(self.input_shape[0], num_filters, 3, bn=True)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, x):\n",
    "        h = F.relu(self.layer0(x))\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, x):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            rp = self(torch.from_numpy(x).unsqueeze(0))\n",
    "        return rp.cpu().numpy()[0]\n",
    "\n",
    "class Prediction(nn.Module):\n",
    "    ''' Policy and value prediction from inner abstract state '''\n",
    "    def __init__(self, action_shape):\n",
    "        super().__init__()\n",
    "        self.board_size = np.prod(action_shape[1:])\n",
    "        self.action_size = action_shape[0] * self.board_size\n",
    "\n",
    "        self.conv_p1 = Conv(num_filters, 4, 1, bn=True)\n",
    "        self.conv_p2 = Conv(4, 1, 1)\n",
    "\n",
    "        self.conv_v = Conv(num_filters, 4, 1, bn=True)\n",
    "        self.fc_v = nn.Linear(self.board_size * 4, 1, bias=False)\n",
    "\n",
    "    def forward(self, rp):\n",
    "        h_p = F.relu(self.conv_p1(rp))\n",
    "        h_p = self.conv_p2(h_p).view(-1, self.action_size)\n",
    "\n",
    "        h_v = F.relu(self.conv_v(rp))\n",
    "        h_v = self.fc_v(h_v.view(-1, self.board_size * 4))\n",
    "\n",
    "        # range of value is -1 ~ 1\n",
    "        return F.softmax(h_p, dim=-1), torch.tanh(h_v)\n",
    "\n",
    "    def inference(self, rp):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            p, v = self(torch.from_numpy(rp).unsqueeze(0))\n",
    "        return p.cpu().numpy()[0], v.cpu().numpy()[0][0]\n",
    "\n",
    "class Dynamics(nn.Module):\n",
    "    '''Abstruct state transition'''\n",
    "    def __init__(self, rp_shape, act_shape):\n",
    "        super().__init__()\n",
    "        self.rp_shape = rp_shape\n",
    "        self.layer0 = Conv(rp_shape[0] + act_shape[0], num_filters, 3, bn=True)\n",
    "        self.blocks = nn.ModuleList([ResidualBlock(num_filters) for _ in range(num_blocks)])\n",
    "\n",
    "    def forward(self, rp, a):\n",
    "        h = torch.cat([rp, a], dim=1)\n",
    "        h = self.layer0(h)\n",
    "        for block in self.blocks:\n",
    "            h = block(h)\n",
    "        return h\n",
    "\n",
    "    def inference(self, rp, a):\n",
    "        self.eval()\n",
    "        with torch.no_grad():\n",
    "            rp = self(torch.from_numpy(rp).unsqueeze(0), torch.from_numpy(a).unsqueeze(0))\n",
    "        return rp.cpu().numpy()[0]\n",
    "\n",
    "class Nets(nn.Module):\n",
    "    '''Whole nets'''\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        state = State()\n",
    "        input_shape = state.feature().shape\n",
    "        action_shape = state.action_feature(0).shape\n",
    "        rp_shape = (num_filters, *input_shape[1:])\n",
    "\n",
    "        self.representation = Representation(input_shape)\n",
    "        self.prediction = Prediction(action_shape)\n",
    "        self.dynamics = Dynamics(rp_shape, action_shape)\n",
    "\n",
    "    def predict_all(self, state0, path):\n",
    "        '''Predict p and v from original state and path'''\n",
    "        outputs = []\n",
    "        self.eval()\n",
    "        x = torch.from_numpy(state0.feature()).unsqueeze(0)\n",
    "        with torch.no_grad():\n",
    "            rp = self.representation(x)\n",
    "            outputs.append(self.prediction(rp))\n",
    "            for action in path:\n",
    "                a = state0.action_feature(action).unsqueeze(0)\n",
    "                rp = self.dynamics(rp, a)\n",
    "                outputs.append(self.prediction(rp))\n",
    "        #  return as numpy arrays\n",
    "        return [(p.cpu().numpy()[0], v.cpu().numpy()[0][0]) for p, v in outputs]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Representation(\n",
       "  (layer0): Conv(\n",
       "    (conv): Conv2d(100, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): ResidualBlock(\n",
       "      (conv): Conv(\n",
       "        (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv): Conv(\n",
       "        (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Representation((100,100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Prediction(\n",
       "  (conv_p1): Conv(\n",
       "    (conv): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (conv_p2): Conv(\n",
       "    (conv): Conv2d(4, 1, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "  )\n",
       "  (conv_v): Conv(\n",
       "    (conv): Conv2d(8, 4, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (fc_v): Linear(in_features=40000, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Prediction((100,100,100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dynamics(\n",
       "  (layer0): Conv(\n",
       "    (conv): Conv2d(200, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "    (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  )\n",
       "  (blocks): ModuleList(\n",
       "    (0): ResidualBlock(\n",
       "      (conv): Conv(\n",
       "        (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "    (1): ResidualBlock(\n",
       "      (conv): Conv(\n",
       "        (conv): Conv2d(8, 8, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
       "        (bn): BatchNorm2d(8, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "      )\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Dynamics((100,100,100),(100,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "j3DEJPlKjMuu"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 2 3\n",
      "A _ _ _\n",
      "B _ _ _\n",
      "C _ _ _\n",
      "record = \n",
      "p = \n",
      "[[[111 111 111]\n",
      "  [111 111 111]\n",
      "  [111 111 111]]]\n",
      "v =  0.0\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def show_net(nets, state):\n",
    "    '''Display policy (p) and value (v)'''\n",
    "    print(state)\n",
    "    p, v = nets.predict_all(state, [])[-1]\n",
    "    print('p = ')\n",
    "    print((p *1000).astype(int).reshape((-1, *nets.representation.input_shape[1:3])))\n",
    "    print('v = ', v)\n",
    "    print()\n",
    "\n",
    "#  Outputs before training\n",
    "show_net(Nets(), State())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "7BlFWmLljMux"
   },
   "outputs": [],
   "source": [
    "# Implementation of Monte Carlo Tree Search\n",
    "\n",
    "class Node:\n",
    "    '''Search result of one abstruct (or root) state'''\n",
    "    def __init__(self, p, v):\n",
    "        self.p, self.v = p, v\n",
    "        self.n, self.q_sum = np.zeros_like(p), np.zeros_like(p)\n",
    "        self.n_all, self.q_sum_all = 1, v / 2 # prior\n",
    "\n",
    "    def update(self, action, q_new):\n",
    "        # Update\n",
    "        self.n[action] += 1\n",
    "        self.q_sum[action] += q_new\n",
    "\n",
    "        # Update overall stats\n",
    "        self.n_all += 1\n",
    "        self.q_sum_all += q_new"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Hz9cn3j6jMuz"
   },
   "outputs": [],
   "source": [
    "import time, copy\n",
    "\n",
    "class Tree:\n",
    "    '''Monte Carlo Tree'''\n",
    "    def __init__(self, nets):\n",
    "        self.nets = nets\n",
    "        self.nodes = {}\n",
    "\n",
    "    def search(self, state, path, rp, depth):\n",
    "        # Return predicted value from new state\n",
    "        key = state.record_string()\n",
    "        if len(path) > 0:\n",
    "            key += '|' + ' '.join(map(state.action2str, path))\n",
    "        if key not in self.nodes:\n",
    "            p, v = self.nets.prediction.inference(rp)\n",
    "            self.nodes[key] = Node(p, v)\n",
    "            return v\n",
    "\n",
    "        # State transition by an action selected from bandit\n",
    "        node = self.nodes[key]\n",
    "        p = node.p\n",
    "        mask = np.zeros_like(p)\n",
    "        if depth == 0:\n",
    "            # Add noise to policy on the root node\n",
    "            p = 0.75 * p + 0.25 * np.random.dirichlet([0.15] * len(p))\n",
    "            # On the root node, we choose action only from legal actions\n",
    "            mask[state.legal_actions()] = 1\n",
    "            p *= mask\n",
    "            p /= p.sum() + 1e-16\n",
    "\n",
    "        n, q_sum = 1 + node.n, node.q_sum_all / node.n_all + node.q_sum\n",
    "        ucb = q_sum / n + 2.0 * np.sqrt(node.n_all) * p / n + mask * 4 # PUCB formula\n",
    "        best_action = np.argmax(ucb)\n",
    "\n",
    "        # Search next state by recursively calling this function\n",
    "        representation = self.nets.dynamics.inference(rp, state.action_feature(best_action))\n",
    "        path.append(best_action)\n",
    "        q_new = -self.search(state, path, rp, depth + 1) # With the assumption of changing player by turn\n",
    "        node.update(best_action, q_new)\n",
    "\n",
    "        return q_new\n",
    "\n",
    "    def think(self, state, num_simulations, temperature = 0, show=False):\n",
    "        # End point of MCTS\n",
    "        if show:\n",
    "            print(state)\n",
    "        start, prev_time = time.time(), 0\n",
    "        for _ in range(num_simulations):\n",
    "            self.search(state, [], self.nets.representation.inference(state.feature()), depth=0)\n",
    "\n",
    "            # Display search result on every second\n",
    "            if show:\n",
    "                tmp_time = time.time() - start\n",
    "                if int(tmp_time) > int(prev_time):\n",
    "                    prev_time = tmp_time\n",
    "                    root, pv = self.nodes[state.record_string()], self.pv(state)\n",
    "                    print('%.2f sec. best %s. q = %.4f. n = %d / %d. pv = %s'\n",
    "                          % (tmp_time, state.action2str(pv[0]), root.q_sum[pv[0]] / root.n[pv[0]],\n",
    "                             root.n[pv[0]], root.n_all, ' '.join([state.action2str(a) for a in pv])))\n",
    "\n",
    "        #  Return probability distribution weighted by the number of simulations\n",
    "        n = root = self.nodes[state.record_string()].n + 1\n",
    "        n = (n / np.max(n)) ** (1 / (temperature + 1e-8))\n",
    "        return n / n.sum()\n",
    "\n",
    "    def pv(self, state):\n",
    "        # Return principal variation (action sequence which is considered as the best)\n",
    "        s, pv_seq = copy.deepcopy(state), []\n",
    "        while True:\n",
    "            key = s.record_string()\n",
    "            if key not in self.nodes or self.nodes[key].n.sum() == 0:\n",
    "                break\n",
    "            best_action = sorted([(a, self.nodes[key].n[a]) for a in s.legal_actions()], key=lambda x: -x[1])[0][0]\n",
    "            pv_seq.append(best_action)\n",
    "            s.play(best_action)\n",
    "        return pv_seq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "LrjwSBIejMu1",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   1 2 3\n",
      "A _ _ _\n",
      "B _ _ _\n",
      "C _ _ _\n",
      "record = \n",
      "   1 2 3\n",
      "A O O _\n",
      "B _ _ _\n",
      "C X X _\n",
      "record = A1 C1 A2 C2\n",
      "   1 2 3\n",
      "A _ X O\n",
      "B _ O O\n",
      "C X _ _\n",
      "record = B2 A2 A3 C1 B3\n",
      "   1 2 3\n",
      "A _ X O\n",
      "B _ O _\n",
      "C X _ _\n",
      "record = B2 A2 A3 C1\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([0.25, 0.  , 0.  , 0.25, 0.  , 0.  , 0.  , 0.25, 0.25],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Search with initialized nets\n",
    "\n",
    "tree = Tree(Nets())\n",
    "tree.think(State(), 100, show=True)\n",
    "\n",
    "tree = Tree(Nets())\n",
    "tree.think(State().play('A1 C1 A2 C2'), 200, show=True)\n",
    "\n",
    "tree = Tree(Nets())\n",
    "tree.think(State().play('B2 A2 A3 C1 B3'), 200, show=True)\n",
    "\n",
    "tree = Tree(Nets())\n",
    "tree.think(State().play('B2 A2 A3 C1'), 200, show=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "C27LMOb4jMu4"
   },
   "outputs": [],
   "source": [
    "# Training of neural nets\n",
    "\n",
    "import torch.optim as optim\n",
    "\n",
    "batch_size = 32\n",
    "num_epochs = 30\n",
    "\n",
    "def gen_target(ep, k):\n",
    "    '''Generate inputs and targets for training'''\n",
    "    # path, reward, observation, action, policy\n",
    "    turn_idx = np.random.randint(len(ep[0]))\n",
    "    ps, vs, ax = [], [], []\n",
    "    for t in range(turn_idx, turn_idx + k + 1):\n",
    "        if t < len(ep[0]):\n",
    "            p = ep[4][t]\n",
    "            a = ep[3][t]\n",
    "        else: # state after finishing game\n",
    "            # p is 0 (loss is 0)\n",
    "            p = np.zeros_like(ep[4][-1])\n",
    "            # random action selection\n",
    "            a = np.zeros(np.prod(ep[3][-1].shape), dtype=np.float32)\n",
    "            a[np.random.randint(len(a))] = 1\n",
    "            a = a.reshape(ep[3][-1].shape)\n",
    "        vs.append([ep[1] if t % 2 == 0 else -ep[1]])\n",
    "        ps.append(p)\n",
    "        ax.append(a)\n",
    "        \n",
    "    return ep[2][turn_idx], ax, ps, vs\n",
    "\n",
    "def train(episodes, nets=Nets()):\n",
    "    '''Train neural nets'''\n",
    "    optimizer = optim.SGD(nets.parameters(), lr=1e-3, weight_decay=1e-4, momentum=0.75)\n",
    "    for epoch in range(num_epochs):\n",
    "        p_loss_sum, v_loss_sum = 0, 0\n",
    "        nets.train()\n",
    "        for i in range(0, len(episodes), batch_size):\n",
    "            k = 4#np.random.randint(4)\n",
    "            x, ax, p_target, v_target = zip(*[gen_target(episodes[np.random.randint(len(episodes))], k) for j in range(batch_size)])\n",
    "            x = torch.from_numpy(np.array(x))\n",
    "            ax = torch.from_numpy(np.array(ax))\n",
    "            p_target = torch.from_numpy(np.array(p_target))\n",
    "            v_target = torch.FloatTensor(np.array(v_target))\n",
    "            \n",
    "            # Change the order of axis as [time step, batch, ...]\n",
    "            ax = torch.transpose(ax, 0, 1)\n",
    "            p_target = torch.transpose(p_target, 0, 1)\n",
    "            v_target = torch.transpose(v_target, 0, 1)\n",
    "\n",
    "            p_loss, v_loss = 0, 0\n",
    "\n",
    "            # Compute losses for k (+ current) steps\n",
    "            for t in range(k + 1):\n",
    "                rp = nets.representation(x) if t == 0 else nets.dynamics(rp, ax[t - 1])\n",
    "                p, v = nets.prediction(rp)\n",
    "                p_loss += torch.sum(-p_target[t] * torch.log(p))\n",
    "                v_loss += torch.sum((v_target[t] - v) ** 2)\n",
    "\n",
    "            p_loss_sum += p_loss.item()\n",
    "            v_loss_sum += v_loss.item()\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            (p_loss + v_loss).backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] *= 0.85\n",
    "    print('p_loss %f v_loss %f' % (p_loss_sum / len(episodes), v_loss_sum / len(episodes)))\n",
    "    return nets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "z6FgL4PxjMu6"
   },
   "outputs": [],
   "source": [
    "#  Battle against random agents\n",
    "\n",
    "def vs_random(nets, n=100):\n",
    "    results = {}\n",
    "    for i in range(n):\n",
    "        first_turn = i % 2 == 0\n",
    "        turn = first_turn\n",
    "        state = State()\n",
    "        while not state.terminal():\n",
    "            if turn:\n",
    "                p, _ = nets.predict_all(state, [])[-1]\n",
    "                action = sorted([(a, p[a]) for a in state.legal_actions()], key=lambda x:-x[1])[0][0]\n",
    "            else:\n",
    "                action = np.random.choice(state.legal_actions())\n",
    "            state.play(action)\n",
    "            turn = not turn\n",
    "        r = state.terminal_reward() if turn else -state.terminal_reward()\n",
    "        results[r] = results.get(r, 0) + 1\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-XpXESW-jMu-",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vs_random =  [(-1, 30), (0, 7), (1, 63)]\n",
      "game 0  1  2  3  4  5  6  7  8  9  generated =  [(-1, 1), (0, 0), (1, 9)]\n",
      "p_loss 23.825948 v_loss 0.366417\n",
      "vs_random =  [(-1, 32), (0, 11), (1, 57)] sum =  [(-1, 62), (0, 18), (1, 120)]\n",
      "game 10  11  12  13  14  15  16  17  18  19  generated =  [(-1, 2), (0, 2), (1, 16)]\n",
      "p_loss 13.751418 v_loss 2.640352\n",
      "vs_random =  [(-1, 31), (0, 5), (1, 64)] sum =  [(-1, 93), (0, 23), (1, 184)]\n",
      "game 20  21  22  23  24  25  26  27  28  29  generated =  [(-1, 2), (0, 4), (1, 24)]\n",
      "p_loss 8.962147 v_loss 1.781910\n",
      "vs_random =  [(-1, 45), (0, 12), (1, 43)] sum =  [(-1, 138), (0, 35), (1, 227)]\n",
      "game 30  31  32  33  34  35  36  37  38  39  generated =  [(-1, 5), (0, 4), (1, 31)]\n",
      "p_loss 12.601338 v_loss 2.741167\n",
      "vs_random =  [(-1, 41), (0, 10), (1, 49)] sum =  [(-1, 179), (0, 45), (1, 276)]\n",
      "game 40  41  42  43  44  45  46  47  48  49  generated =  [(-1, 7), (0, 4), (1, 39)]\n",
      "p_loss 9.652631 v_loss 2.243840\n",
      "vs_random =  [(-1, 34), (0, 12), (1, 54)] sum =  [(-1, 213), (0, 57), (1, 330)]\n",
      "game 50  51  52  53  54  55  56  57  58  59  generated =  [(-1, 10), (0, 8), (1, 42)]\n",
      "p_loss 7.343138 v_loss 2.788658\n",
      "vs_random =  [(-1, 43), (0, 15), (1, 42)] sum =  [(-1, 256), (0, 72), (1, 372)]\n",
      "game 60  61  62  63  64  65  66  67  68  69  generated =  [(-1, 14), (0, 8), (1, 48)]\n",
      "p_loss 8.840349 v_loss 3.323356\n",
      "vs_random =  [(-1, 40), (0, 10), (1, 50)] sum =  [(-1, 296), (0, 82), (1, 422)]\n",
      "game 70  71  72  73  74  75  76  77  78  79  generated =  [(-1, 16), (0, 10), (1, 54)]\n",
      "p_loss 6.912940 v_loss 2.815507\n",
      "vs_random =  [(-1, 41), (0, 13), (1, 46)] sum =  [(-1, 337), (0, 95), (1, 468)]\n",
      "game 80  81  82  83  84  85  86  87  88  89  generated =  [(-1, 20), (0, 11), (1, 59)]\n",
      "p_loss 6.155144 v_loss 1.951901\n",
      "vs_random =  [(-1, 36), (0, 19), (1, 45)] sum =  [(-1, 373), (0, 114), (1, 513)]\n",
      "game 90  91  92  93  94  95  96  97  98  99  generated =  [(-1, 24), (0, 13), (1, 63)]\n",
      "p_loss 7.085295 v_loss 3.187228\n",
      "vs_random =  [(-1, 40), (0, 14), (1, 46)] sum =  [(-1, 413), (0, 128), (1, 559)]\n",
      "game 100  101  102  103  104  105  106  107  108  109  generated =  [(-1, 29), (0, 15), (1, 66)]\n",
      "p_loss 6.040501 v_loss 3.000500\n",
      "vs_random =  [(-1, 44), (0, 14), (1, 42)] sum =  [(-1, 457), (0, 142), (1, 601)]\n",
      "game 110  111  112  113  114  115  116  117  118  119  generated =  [(-1, 31), (0, 16), (1, 73)]\n",
      "p_loss 5.665408 v_loss 2.783212\n",
      "vs_random =  [(-1, 37), (0, 16), (1, 47)] sum =  [(-1, 494), (0, 158), (1, 648)]\n",
      "game 120  121  122  123  124  125  "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-10-10c93e66e16e>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     21\u001b[0m     \u001b[0;32mwhile\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mterminal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     22\u001b[0m         \u001b[0mtree\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 23\u001b[0;31m         \u001b[0mp_target\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mthink\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_simulations\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtemperature\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     24\u001b[0m         \u001b[0mp_targets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mp_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m         \u001b[0mfeatures\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dd8770e76eb6>\u001b[0m in \u001b[0;36mthink\u001b[0;34m(self, state, num_simulations, temperature, show)\u001b[0m\n\u001b[1;32m     47\u001b[0m         \u001b[0mstart\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprev_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0m_\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_simulations\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 49\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrepresentation\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfeature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     50\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     51\u001b[0m             \u001b[0;31m# Display search result on every second\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dd8770e76eb6>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, state, path, rp, depth)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mrepresentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mq_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# With the assumption of changing player by turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dd8770e76eb6>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, state, path, rp, depth)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mrepresentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mq_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# With the assumption of changing player by turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dd8770e76eb6>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, state, path, rp, depth)\u001b[0m\n\u001b[1;32m     36\u001b[0m         \u001b[0mrepresentation\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnets\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdynamics\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minference\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m         \u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 38\u001b[0;31m         \u001b[0mq_new\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msearch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpath\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdepth\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# With the assumption of changing player by turn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     39\u001b[0m         \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_action\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_new\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-6-dd8770e76eb6>\u001b[0m in \u001b[0;36msearch\u001b[0;34m(self, state, path, rp, depth)\u001b[0m\n\u001b[1;32m     31\u001b[0m         \u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m1\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_sum_all\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_all\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mq_sum\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m         \u001b[0mucb\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mq_sum\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m2.0\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msqrt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn_all\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mp\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0mn\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmask\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m4\u001b[0m \u001b[0;31m# PUCB formula\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m         \u001b[0mbest_action\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mucb\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m         \u001b[0;31m# Search next state by recursively calling this function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# Main algorithm of MuZero\n",
    "\n",
    "num_games = 50000\n",
    "num_train_steps = 10\n",
    "num_simulations = 30\n",
    "\n",
    "nets = Nets()\n",
    "\n",
    "# Display battle results as {-1: lose 0: draw 1: win} (for episode generated for training, 1 means that the first player won)\n",
    "vs_random_sum = vs_random(nets)\n",
    "print('vs_random = ', sorted(vs_random_sum.items()))\n",
    "\n",
    "episodes = []\n",
    "result_distribution = {1:0, 0:0, -1:0}\n",
    "\n",
    "for g in range(num_games):\n",
    "    # Generate one 1 episode\n",
    "    record, p_targets, features, action_features = [], [], [], []\n",
    "    state = State()\n",
    "    temperature = 0.7 # temperature using to make policy targets from search results\n",
    "    while not state.terminal():\n",
    "        tree = Tree(nets)\n",
    "        p_target = tree.think(state, num_simulations, temperature)\n",
    "        p_targets.append(p_target)\n",
    "        features.append(state.feature())\n",
    "        # Select action with generated distribution, and then make a transition by that action\n",
    "        action = np.random.choice(np.arange(len(p_target)), p=p_target)\n",
    "        action_features.append(state.action_feature(action))\n",
    "        state.play(action)\n",
    "        record.append(action)\n",
    "        temperature *= 0.8\n",
    "    # reward seen from the first turn player\n",
    "    reward = state.terminal_reward() * (1 if len(record) % 2 == 0 else -1)\n",
    "    result_distribution[reward] += 1\n",
    "    episodes.append((record, reward, features, action_features, p_targets))\n",
    "    if g % num_train_steps == 0:\n",
    "        print('game ', end='')\n",
    "    print(g, ' ', end='')\n",
    "\n",
    "    # Training of neural nets\n",
    "    if (g + 1) % num_train_steps == 0:\n",
    "        # Show the result distributiuon of generated episodes\n",
    "        print('generated = ', sorted(result_distribution.items()))\n",
    "        nets = train(episodes, nets)\n",
    "        vs_random_once = vs_random(nets)\n",
    "        print('vs_random = ', sorted(vs_random_once.items()), end='')\n",
    "        for r, n in vs_random_once.items():\n",
    "            vs_random_sum[r] += n\n",
    "        print(' sum = ', sorted(vs_random_sum.items()))\n",
    "        #show_net(nets, State())\n",
    "        #show_net(nets, State().play('A1 C1 A2 C2'))\n",
    "        #show_net(nets, State().play('A1 B2 C3 B3 C1'))\n",
    "        #show_net(nets, State().play('B2 A2 A3 C1 B3'))\n",
    "        #show_net(nets, State().play('B2 A2 A3 C1'))\n",
    "print('finished')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wfxSo5L0jMvA"
   },
   "outputs": [],
   "source": [
    "# Show outputs from trained nets\n",
    "\n",
    "print('initial state')\n",
    "show_net(nets, State())\n",
    "\n",
    "print('WIN by put')\n",
    "show_net(nets, State().play('A1 C1 A2 C2'))\n",
    "\n",
    "print('LOSE by opponent\\'s double')\n",
    "show_net(nets, State().play('B2 A2 A3 C1 B3'))\n",
    "\n",
    "print('WIN through double')\n",
    "show_net(nets, State().play('B2 A2 A3 C1'))\n",
    "\n",
    "# hard case: putting on A1 will cause double\n",
    "print('strategic WIN by following double')\n",
    "show_net(nets, State().play('B1 A3'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WZ96jAAqjMvD"
   },
   "outputs": [],
   "source": [
    "# Search with trained nets\n",
    "\n",
    "tree = Tree(net)\n",
    "tree.think(State(), 100000, show=True)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "name": "MuZeroExample.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "generalML",
   "language": "python",
   "name": "generalml"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
